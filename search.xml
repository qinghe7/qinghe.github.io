<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Tianshou</title>
      <link href="/2023/08/27/Tianshou/"/>
      <url>/2023/08/27/Tianshou/</url>
      
        <content type="html"><![CDATA[<p>source：<a href="https://zhuanlan.zhihu.com/p/620988786">Tianshou–强化学习算法框架学习笔记 - 知乎 (zhihu.com)</a></p><p><a href="https://tianshou.readthedocs.io/en/master/tutorials/cheatsheet.html">Cheat Sheet — Tianshou 0.5.1 documentation</a></p><p><a href="https://tianshou.readthedocs.io/zh/master/">欢迎查看天授平台中文文档 — 天授 0.4.6.post1 文档 (tianshou.readthedocs.io)</a></p><h3 id="一、Tianshou的基本框架"><a href="#一、Tianshou的基本框架" class="headerlink" title="一、Tianshou的基本框架"></a><strong>一、Tianshou的基本框架</strong></h3><p>天授（Tianshou）把一个RL训练流程划分成了几个子模块：trainer（负责训练逻辑）、collector（负责数据采集）、policy（负责训练策略）和 buffer（负责数据存储），此外还有两个外围的模块，一个是env，一个是model（policy负责RL算法实现比如loss function的计算，model就只是个正常的神经网络）。下图描述了这些模块的依赖：</p><p><img src="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp" alt="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp"></p><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tianshou <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> tianshou.utils <span class="keyword">import</span> TensorboardLogger</span><br><span class="line"> ​</span><br><span class="line"> ​</span><br><span class="line"> <span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_shape, action_shape</span>):</span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = nn.Sequential(</span><br><span class="line">             nn.Linear(np.prod(state_shape), <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, np.prod(action_shape)),</span><br><span class="line">         )</span><br><span class="line"> ​</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, obs, state=<span class="literal">None</span>, info=&#123;&#125;</span>):</span><br><span class="line">         <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obs, torch.Tensor):</span><br><span class="line">             obs = torch.tensor(obs, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">         batch = obs.shape[<span class="number">0</span>]</span><br><span class="line">         logits = self.model(obs.view(batch, -<span class="number">1</span>))</span><br><span class="line">         <span class="keyword">return</span> logits, state</span><br><span class="line"> ​</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Make an environment</span></span><br><span class="line"> env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line"> state_shape = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"> action_shape = env.action_space.n</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Build the Network</span></span><br><span class="line"> net = Net(state_shape, action_shape)</span><br><span class="line"> optim = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Setup Policy</span></span><br><span class="line"> policy = ts.policy.DQNPolicy(net, optim, discount_factor=<span class="number">0.9</span>, estimation_step=<span class="number">3</span>, target_update_freq=<span class="number">320</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Setup Collector</span></span><br><span class="line"> train_collector = ts.data.Collector(policy, env, ts.data.ReplayBuffer(<span class="number">20000</span>, <span class="number">10</span>), exploration_noise=<span class="literal">True</span>)</span><br><span class="line"> test_collector = ts.data.Collector(policy, env, exploration_noise=<span class="literal">True</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># logging</span></span><br><span class="line"> <span class="comment"># writer = SummaryWriter(&#x27;log/dqn&#x27;)</span></span><br><span class="line"> <span class="comment"># logger = TensorboardLogger(writer)</span></span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Train Policy with a Trainer</span></span><br><span class="line"> result = ts.trainer.offpolicy_trainer(</span><br><span class="line">     policy, train_collector, test_collector,</span><br><span class="line">     max_epoch=<span class="number">10</span>, step_per_epoch=<span class="number">10000</span>, step_per_collect=<span class="number">10</span>,</span><br><span class="line">     update_per_step=<span class="number">0.1</span>, episode_per_test=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">     train_fn=<span class="keyword">lambda</span> epoch, env_step: policy.set_eps(<span class="number">0.1</span>),</span><br><span class="line">     test_fn=<span class="keyword">lambda</span> epoch, env_step: policy.set_eps(<span class="number">0.05</span>),</span><br><span class="line">     stop_fn=<span class="keyword">lambda</span> mean_rewards: mean_rewards &gt;= env.spec.reward_threshold)</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">f&#x27;Finished training! Use <span class="subst">&#123;result[<span class="string">&quot;duration&quot;</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>下面，我们就将一步一步的了解上图中所有的API，对Tianshou有一个大致的了解。</p><h3 id="二、Batch"><a href="#二、Batch" class="headerlink" title="二、Batch"></a><strong>二、Batch</strong></h3><p>下面我们首先来看看Batch这个数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch</span><br><span class="line">data = Batch(a=<span class="number">4</span>, b=[<span class="number">5</span>, <span class="number">5</span>], c=<span class="string">&#x27;2312312&#x27;</span>, d=(<span class="string">&#x27;a&#x27;</span>, -<span class="number">2</span>, -<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="built_in">print</span>(data.b)</span><br><span class="line">​</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array(4),</span></span><br><span class="line"><span class="string">    b: array([5, 5]),</span></span><br><span class="line"><span class="string">    c: &#x27;2312312&#x27;,</span></span><br><span class="line"><span class="string">    d: array([&#x27;a&#x27;, &#x27;-2&#x27;, &#x27;-3&#x27;], dtype=object),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">[5 5]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>可以发现，batch类似于dict，存储key-value对，并且可以自动将value转化成numpy array。</p><p>下面例子演示Batch存储numpy和pytorch的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">batch1 = Batch(a=np.arange(<span class="number">2</span>), b=torch.zeros((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">batch2 = Batch(a=np.arange(<span class="number">2</span>), b=torch.ones((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">batch_cat = Batch.cat([batch1, batch2, batch1])</span><br><span class="line"><span class="built_in">print</span>(batch1)</span><br><span class="line"><span class="built_in">print</span>(batch2)</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">    a: array([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>将Batch中的数据类型统一转换成numpy或pytorch的数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">batch_cat.to_numpy()</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line">batch_cat.to_torch()</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">    b: array([[0., 0.],</span></span><br><span class="line"><span class="string">              [0., 0.],</span></span><br><span class="line"><span class="string">              [1., 1.],</span></span><br><span class="line"><span class="string">              [1., 1.],</span></span><br><span class="line"><span class="string">              [0., 0.],</span></span><br><span class="line"><span class="string">              [0., 0.]], dtype=float32),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: tensor([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="三、ReplayBuffer"><a href="#三、ReplayBuffer" class="headerlink" title="三、ReplayBuffer"></a><strong>三、ReplayBuffer</strong></h3><p>Replay buffer在RL的off-policy中非常常用，其可以存储过去的经验，以便训练我们的agent。</p><p>在Tianshou中，可以把replay buffer看成一种特殊的Batch。</p><p>下面是使用replay buffer的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer</span><br><span class="line">​</span><br><span class="line"><span class="comment"># a buffer is initialised with its maxsize set to 10 (older data will be discarded if more data flow in).</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line">buf = ReplayBuffer(size=<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># add 3 steps of data into ReplayBuffer sequentially</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    buf.add(Batch(obs=i, act=i, rew=i, done=<span class="number">0</span>, obs_next=i + <span class="number">1</span>, info=&#123;&#125;))</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># add another 10 steps of data into ReplayBuffer sequentially</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, <span class="number">13</span>):</span><br><span class="line">    buf.add(Batch(obs=i, act=i, rew=i, done=<span class="number">0</span>, obs_next=i + <span class="number">1</span>, info=&#123;&#125;))</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer()</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 0</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer(</span></span><br><span class="line"><span class="string">    info: Batch(),</span></span><br><span class="line"><span class="string">    obs_next: array([1, 2, 3, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    act: array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    obs: array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    done: array([False, False, False, False, False, False, False, False, False,</span></span><br><span class="line"><span class="string">                 False]),</span></span><br><span class="line"><span class="string">    rew: array([0., 1., 2., 0., 0., 0., 0., 0., 0., 0.]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 3</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer(</span></span><br><span class="line"><span class="string">    info: Batch(),</span></span><br><span class="line"><span class="string">    obs_next: array([11, 12, 13,  4,  5,  6,  7,  8,  9, 10]),</span></span><br><span class="line"><span class="string">    act: array([10, 11, 12,  3,  4,  5,  6,  7,  8,  9]),</span></span><br><span class="line"><span class="string">    obs: array([10, 11, 12,  3,  4,  5,  6,  7,  8,  9]),</span></span><br><span class="line"><span class="string">    done: array([False, False, False, False, False, False, False, False, False,</span></span><br><span class="line"><span class="string">                 False]),</span></span><br><span class="line"><span class="string">    rew: array([10., 11., 12.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 10</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>replay buffer中保留了七个属性，Tianshou推荐我们使用这七个推荐的属性，而不是自己去创建其他属性。</p><p><img src="https://pic3.zhimg.com/80/v2-44dbd0b48689e7816dd0701523d7d9be_720w.webp"></p><p>我们也看到了，buffer其实就是一种特殊的Batch，那他存在的意义是什么呢？</p><p>就在于可以从buffer中sample数据给到collector中，供agent进行训练。</p><blockquote><p>现在Tianshou支持gymnasium，所以又多了两个属性：truncated和terminated。</p></blockquote><p>我们还可以高效的从buffer中追踪trajectory信息。</p><p>下面这段代码可以获得下标为6的step所处的episode的第一个step的下标</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Search for the previous index of index &quot;6&quot;</span></span><br><span class="line">now_index = <span class="number">6</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    prev_index = buf.prev(now_index)</span><br><span class="line">    <span class="built_in">print</span>(prev_index)</span><br><span class="line">    <span class="keyword">if</span> prev_index == now_index:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        now_index = prev_index</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">4</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>同理，下面代码可以返回在当前episode中下一个step的下标:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># next step of indexes [4,5,6,7,8,9] are:</span></span><br><span class="line"><span class="built_in">print</span>(buf.<span class="built_in">next</span>([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]))</span><br><span class="line"><span class="built_in">print</span>(buf.<span class="built_in">next</span>(<span class="number">7</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[5 6 7 7 9 0]</span></span><br><span class="line"><span class="string">7</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>这在n-step-return的时候非常有用(n-step TD)</p><h3 id="四、Vectorized-Environment"><a href="#四、Vectorized-Environment" class="headerlink" title="四、Vectorized Environment"></a><strong>四、Vectorized Environment</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-d7073830e1c3abca3458de2d42f3a37e_720w.webp"></p><p>在gym中，环境接收一个动作，返回下一个状态的观测和奖励。这个过程很慢，并且常常是实验的性能瓶颈，所以Tianshou利用并行环境加速这一过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> SubprocVectorEnv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">​</span><br><span class="line">num_cpus = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">for</span> num_cpu <span class="keyword">in</span> num_cpus:</span><br><span class="line">    <span class="comment"># SubprocVectorEnv这个wrapper利用多个进程，并行执行多个环境。</span></span><br><span class="line">    env = SubprocVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_cpu)])</span><br><span class="line">    env.reset()</span><br><span class="line">    sampled_steps = <span class="number">0</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line">    <span class="keyword">while</span> sampled_steps &lt; <span class="number">1000</span>:</span><br><span class="line">        act = np.random.choice(<span class="number">2</span>, size=num_cpu)</span><br><span class="line">        obs, rew, done, info = env.step(act)</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">sum</span>(done):</span><br><span class="line">            env.reset(np.where(done)[<span class="number">0</span>])</span><br><span class="line">        sampled_steps += num_cpu</span><br><span class="line">    time_used = time.time() - time_start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;s used to sample 1000 steps if using &#123;&#125; cpus.&quot;</span>.<span class="built_in">format</span>(time_used, num_cpu))</span><br></pre></td></tr></table></figure><p>下面是单个环境与多个环境的对比：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="comment"># In Gym</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Tianshou</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">helper_function</span>():</span><br><span class="line">    env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">  <span class="comment"># other operations such as env.seed(np.random.choice(10))</span></span><br><span class="line">    <span class="keyword">return</span> env</span><br><span class="line">​</span><br><span class="line">envs = DummyVectorEnv([helper_function <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Gym, env.reset() returns a single observation.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In Gym, env.reset() returns a single observation.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(env.reset())</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Tianshou, envs.reset() returns stacked observations.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In Tianshou, envs.reset() returns stacked observations.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(envs.reset())</span><br><span class="line">​</span><br><span class="line">obs, rew, done, info = envs.step(np.random.choice(<span class="number">2</span>, size=num_cpu))</span><br><span class="line"><span class="built_in">print</span>(info)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">In Gym, env.reset() returns a single observation.</span></span><br><span class="line"><span class="string">[0.04703292 0.03945684 0.03802961 0.02598534]</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">In Tianshou, envs.reset() returns stacked observations.</span></span><br><span class="line"><span class="string">[[ 0.04029649 -0.01946092 -0.02980652 -0.01614117]</span></span><br><span class="line"><span class="string"> [-0.03085166 -0.04178732 -0.02325586  0.00156881]</span></span><br><span class="line"><span class="string"> [ 0.00672287  0.04306572  0.01217845 -0.04455359]</span></span><br><span class="line"><span class="string"> [ 0.03829754  0.02683093 -0.01153483  0.04290532]</span></span><br><span class="line"><span class="string"> [ 0.04420044  0.00097068 -0.01117315  0.04102308]]</span></span><br><span class="line"><span class="string">[&#123;&#x27;env_id&#x27;: 0&#125; &#123;&#x27;env_id&#x27;: 1&#125; &#123;&#x27;env_id&#x27;: 2&#125; &#123;&#x27;env_id&#x27;: 3&#125; &#123;&#x27;env_id&#x27;: 4&#125;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="五、Policy"><a href="#五、Policy" class="headerlink" title="五、Policy"></a><strong>五、Policy</strong></h3><p>Policy就是agent如何做出action的\pi函数。</p><p>所有Policy模块都继承自BasePolicy类，并且具有相同的接口。</p><p>下面我们就来看看如何实现一个简单的REINFORCE的policy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Dict</span>, <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Type</span>, <span class="type">Union</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer, to_torch, to_torch_as</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> BasePolicy</span><br><span class="line">​</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">REINFORCEPolicy</span>(<span class="title class_ inherited__">BasePolicy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implementation of REINFORCE algorithm.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br></pre></td></tr></table></figure><p>policy最重要的两个功能就是</p><ol><li>选择动作(forward)</li><li>更新参数(update)，update先调用process_fn函数，处理从buffer来的数据；然后调用learn，反向传播，更新参数。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Dict</span>, <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Type</span>, <span class="type">Union</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer, to_torch, to_torch_as</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> BasePolicy</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">REINFORCEPolicy</span>(<span class="title class_ inherited__">BasePolicy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implementation of REINFORCE algorithm.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model: torch.nn.Module, optim: torch.optim.Optimizer,</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.actor = model</span><br><span class="line">        self.optim = optim</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Batch</span>) -&gt; Batch:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute action over the given batch data.&quot;&quot;&quot;</span></span><br><span class="line">        act = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> Batch(act=act)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_fn</span>(<span class="params">self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray</span>) -&gt; Batch:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the discounted returns for each transition.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self, batch: Batch, batch_size: <span class="built_in">int</span>, repeat: <span class="built_in">int</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">float</span>]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Perform the back-propagation.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure><h3 id="六、Collector"><a href="#六、Collector" class="headerlink" title="六、Collector"></a><strong>六、Collector</strong></h3><p>collector与policy和环境交互，在其内部，把envs和buffer有机的结合起来，封装了其中的数据交互。</p><p><img src="https://pic3.zhimg.com/80/v2-1248cd71b07e48326fc65217097156f6_720w.webp"></p><p>Collector在训练（收集数据）时和评估策略时都可以使用。</p><p>Data Collecting：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> VectorReplayBuffer</span><br><span class="line">​</span><br><span class="line">train_env_num = <span class="number">4</span></span><br><span class="line">buffer_size = <span class="number">100</span></span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v0&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(train_env_num)])</span><br><span class="line">replaybuffer = VectorReplayBuffer(buffer_size, train_env_num)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 定义一个Collector</span></span><br><span class="line">train_collector = Collector(policy, train_envs, replaybuffer)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 利用Collector收集50个step的数据，自动存入replaybuffer中</span></span><br><span class="line">collect_result = train_collector.collect(n_step=<span class="number">50</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面我们可以从buffer中抽样数据</span></span><br><span class="line">replaybuffer.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>Policy evaluation：</p><p>我们已经有了一个policy，现在我们想评估一下这个policy，看看reward情况等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PGPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v0&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">16</span>,])</span><br><span class="line">actor = Actor(net, env.action_space.shape)</span><br><span class="line">optim = torch.optim.Adam(actor.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line">​</span><br><span class="line">policy = PGPolicy(actor, optim, dist_fn=torch.distributions.Categorical)</span><br><span class="line">test_collector = Collector(policy, test_envs)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 收集9个episode</span></span><br><span class="line">collect_result = test_collector.collect(n_episode=<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(collect_result)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;n/ep&#x27;: 9, &#x27;n/st&#x27;: 82, &#x27;rews&#x27;: array([ 9.,  9.,  9.,  9.,  8.,  9.,  9., 11.,  9.]), &#x27;lens&#x27;: array([ 9,  9,  9,  9,  8,  9,  9, 11,  9]), &#x27;idxs&#x27;: array([0, 1, 0, 1, 0, 1, 0, 1, 0]), &#x27;rew&#x27;: 9.11111111111111, &#x27;len&#x27;: 9.11111111111111, &#x27;rew_std&#x27;: 0.7370277311900889, &#x27;len_std&#x27;: 0.7370277311900889&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rewards of 9 episodes are &#123;&#125;&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;rews&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Rewards of 9 episodes are [ 9.  9.  9.  9.  8.  9.  9. 11.  9.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average episode reward is &#123;&#125;.&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;rew&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Average episode reward is 9.11111111111111.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average episode length is &#123;&#125;.&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;len&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Average episode length is 9.11111111111111.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="七、Trainer"><a href="#七、Trainer" class="headerlink" title="七、Trainer"></a><strong>七、Trainer</strong></h3><p>Trainer是Tianshou中的顶层封装，它控制traning loop和对Policy的evaluation。Trainer控制Policy和Collector的交互。</p><p>Tianshou中包含三类Trainer：On-policy training, off-policy training, offline training.</p><p>下面是REINFORCE算法的整体流程(利用On-policy)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector, VectorReplayBuffer</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PGPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">​</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">train_env_num = <span class="number">4</span></span><br><span class="line">buffer_size = <span class="number">2000</span>  <span class="comment"># Since REINFORCE is an on-policy algorithm, we don&#x27;t need a very large buffer size</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the environments, used for training and evaluation</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>)</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(train_env_num)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the Policy instance</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">16</span>, ])</span><br><span class="line">actor = Actor(net, env.action_space.shape)</span><br><span class="line">optim = torch.optim.Adam(actor.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">policy = PGPolicy(actor, optim, dist_fn=torch.distributions.Categorical)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the replay buffer and the collector</span></span><br><span class="line">replaybuffer = VectorReplayBuffer(buffer_size, train_env_num)</span><br><span class="line">test_collector = Collector(policy, test_envs)  <span class="comment"># 可以发现，test_collector没有replaybuffer，因为不做训练，只是测试</span></span><br><span class="line">train_collector = Collector(policy, train_envs, replaybuffer)</span><br><span class="line">​</span><br><span class="line">train_collector.reset()</span><br><span class="line">train_envs.reset()</span><br><span class="line">test_collector.reset()</span><br><span class="line">test_envs.reset()</span><br><span class="line">replaybuffer.reset()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 10 epoch</span></span><br><span class="line">    evaluation_result = test_collector.collect(n_episode=<span class="number">10</span>)  <span class="comment"># test_collector用来测试当前policy，得出reward。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evaluation reward is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(evaluation_result[<span class="string">&quot;rew&quot;</span>]))</span><br><span class="line">    train_collector.collect(n_step=<span class="number">2000</span>)  <span class="comment"># 收集2000个step到replaybuffer中</span></span><br><span class="line">    <span class="comment"># 0 means taking all data stored in train_collector.buffer</span></span><br><span class="line">    policy.update(<span class="number">0</span>, train_collector.buffer, batch_size=<span class="number">512</span>, repeat=<span class="number">1</span>)  <span class="comment"># buffer中所有数据，每次batch_size为512，</span></span><br><span class="line">    train_collector.reset_buffer(keep_statistics=<span class="literal">True</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面是使用tainer的代码：</span></span><br><span class="line">train_collector.reset()</span><br><span class="line">train_envs.reset()</span><br><span class="line">test_collector.reset()</span><br><span class="line">test_envs.reset()</span><br><span class="line">replaybuffer.reset()</span><br><span class="line">​</span><br><span class="line">result = onpolicy_trainer(</span><br><span class="line">    policy,</span><br><span class="line">    train_collector,</span><br><span class="line">    test_collector,</span><br><span class="line">    max_epoch=<span class="number">10</span>,</span><br><span class="line">    step_per_epoch=<span class="number">1</span>,  <span class="comment"># 每个epoch进行多少次transitions</span></span><br><span class="line">    repeat_per_collect=<span class="number">1</span>, <span class="comment"># the number of repeat time for policy learning, for example, set it to 2 means the policy needs to learn each given batch data twice.</span></span><br><span class="line">    episode_per_test=<span class="number">10</span>,  <span class="comment"># 每次测试进行几个episode</span></span><br><span class="line">    step_per_collect=<span class="number">2000</span>, <span class="comment"># 每次update前，收集多少step的数据</span></span><br><span class="line">    batch_size=<span class="number">512</span>,  <span class="comment"># update的时候batch的大小</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>可以发现，trainer就是包装了一下循环。</p><h3 id="八、Experiment"><a href="#八、Experiment" class="headerlink" title="八、Experiment"></a><strong>八、Experiment</strong></h3><p>这一节我们用PPO来解决CartPole</p><p><img src="https://pic1.zhimg.com/80/v2-e0d5a938b90f41c1ddd9679ae5cad078_720w.webp"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector, VectorReplayBuffer</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PPOPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.trainer <span class="keyword">import</span> onpolicy_trainer</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> ActorCritic, Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor, Critic</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">​</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)])</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># net is the shared head of the actor and the critic</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">64</span>, <span class="number">64</span>], device=device)</span><br><span class="line">actor = Actor(net, env.action_space.n, device=device).to(device)</span><br><span class="line">critic = Critic(net, device=device).to(device)</span><br><span class="line">actor_critic = ActorCritic(actor, critic)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># optimizer of the actor and the critic</span></span><br><span class="line">optim = torch.optim.Adam(actor_critic.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line">​</span><br><span class="line">dist = torch.distributions.Categorical</span><br><span class="line">policy = PPOPolicy(actor, critic, optim, dist, action_space=env.action_space, deterministic_eval=<span class="literal">True</span>)</span><br><span class="line">​</span><br><span class="line">train_collector = Collector(policy, train_envs, VectorReplayBuffer(<span class="number">20000</span>, <span class="built_in">len</span>(train_envs)))</span><br><span class="line">test_collector = Collector(policy, test_envs)</span><br><span class="line">​</span><br><span class="line">result = onpolicy_trainer(</span><br><span class="line">    policy,</span><br><span class="line">    train_collector,</span><br><span class="line">    test_collector,</span><br><span class="line">    max_epoch=<span class="number">10</span>,</span><br><span class="line">    step_per_epoch=<span class="number">50000</span>,</span><br><span class="line">    repeat_per_collect=<span class="number">10</span>,</span><br><span class="line">    episode_per_test=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    step_per_collect=<span class="number">2000</span>,</span><br><span class="line">    stop_fn=<span class="keyword">lambda</span> mean_reward: mean_reward &gt;= <span class="number">195</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Reinforement learning </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>My New Post</title>
      <link href="/2023/08/22/vue/"/>
      <url>/2023/08/22/vue/</url>
      
        <content type="html"><![CDATA[<p>vue 响应式界面真的很棒</p><p><a href="https://v2.cn.vuejs.org/v2/guide/">介绍 — Vue.js (vuejs.org)</a></p><h1 id="深入响应式系统深入响应式系统-Vue-js-vuejs-org"><a href="#深入响应式系统深入响应式系统-Vue-js-vuejs-org" class="headerlink" title="深入响应式系统深入响应式系统 | Vue.js (vuejs.org)"></a>深入响应式系统<a href="https://cn.vuejs.org/guide/extras/reactivity-in-depth.html#how-reactivity-works-in-vue">深入响应式系统 | Vue.js (vuejs.org)</a></h1><p>Vue 最标志性的功能就是其低侵入性的响应式系统。组件状态都是由响应式的 JavaScript 对象组成的。当更改它们时，视图会随即自动更新。这让状态管理更加简单直观，但理解它是如何工作的也是很重要的，这可以帮助我们避免一些常见的陷阱。在本节中，我们将深入研究 Vue 响应性系统的一些底层细节。</p><p>这个术语在今天的各种编程讨论中经常出现，但人们说它的时候究竟是想表达什么意思呢？本质上，响应性是一种可以使我们声明式地处理变化的编程范式。一个经常被拿来当作典型例子的用例即是 Excel 表格：</p><table><thead><tr><th></th><th>A</th><th>B</th><th>C</th></tr></thead><tbody><tr><td>0</td><td></td><td></td><td></td></tr><tr><td>1</td><td></td><td></td><td></td></tr><tr><td>2</td><td></td><td></td><td></td></tr></tbody></table><p>这里单元格 A2 中的值是通过公式 <code>= A0 + A1</code> 来定义的 (你可以在 A2 上点击来查看或编辑该公式)，因此最终得到的值为 3，正如所料。但如果你试着更改 A0 或 A1，你会注意到 A2 也随即自动更新了。</p><p>而 JavaScript 默认并不是这样的。如果我们用 JavaScript 写类似的逻辑：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> <span class="variable constant_">A0</span> = <span class="number">1</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable constant_">A1</span> = <span class="number">2</span></span><br><span class="line"><span class="keyword">let</span> <span class="variable constant_">A2</span> = <span class="variable constant_">A0</span> + <span class="variable constant_">A1</span></span><br><span class="line"></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="variable constant_">A2</span>) <span class="comment">// 3</span></span><br><span class="line"></span><br><span class="line"><span class="variable constant_">A0</span> = <span class="number">2</span></span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="variable constant_">A2</span>) <span class="comment">// 仍然是 3</span></span><br></pre></td></tr></table></figure><p>当我们更改 <code>A0</code> 后，<code>A2</code> 不会自动更新。</p><p>那么我们如何在 JavaScript 中做到这一点呢？首先，为了能重新运行计算的代码来更新 <code>A2</code>，我们需要将其包装为一个函数</p><p>let A2</p><p>function update() {<br>  A2 &#x3D; A0 + A1<br>}</p><p>然后，我们需要定义几个术语：</p><ul><li>这个 <code>update()</code> 函数会产生一个<strong>副作用</strong>，或者就简称为<strong>作用</strong> (effect)，因为它会更改程序里的状态。</li><li><code>A0</code> 和 <code>A1</code> 被视为这个作用的<strong>依赖</strong> (dependency)，因为它们的值被用来执行这个作用。因此这次作用也可以说是一个它依赖的<strong>订阅者</strong> (subscriber)。</li></ul><p>我们需要一个魔法函数，能够在 <code>A0</code> 或 <code>A1</code> (这两个<strong>依赖</strong>) 变化时调用 <code>update()</code> (产生<strong>作用</strong>)。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_">whenDepsChange</span>(update)</span><br></pre></td></tr></table></figure><p>这个 <code>whenDepsChange()</code> 函数有如下的任务：</p><ol><li>当一个变量被读取时进行追踪。例如我们执行了表达式 <code>A0 + A1</code> 的计算，则 <code>A0</code> 和 <code>A1</code> 都被读取到了。</li><li>如果一个变量在当前运行的副作用中被读取了，就将该副作用设为此变量的一个订阅者。例如由于 <code>A0</code> 和 <code>A1</code> 在 <code>update()</code> 执行时被访问到了，则 <code>update()</code> 需要在第一次调用之后成为 <code>A0</code> 和 <code>A1</code> 的订阅者。</li><li>探测一个变量的变化。例如当我们给 <code>A0</code> 赋了一个新的值后，应该通知其所有订阅了的副作用重新执行。</li></ol><h1 id="Vue-中的响应性是如何工作的"><a href="#Vue-中的响应性是如何工作的" class="headerlink" title="Vue 中的响应性是如何工作的"></a>Vue 中的响应性是如何工作的</h1><p>我们无法直接追踪对上述示例中局部变量的读写，原生 JavaScript 没有提供任何机制能做到这一点。<strong>但是</strong>，我们是可以追踪<strong>对象属性</strong>的读写的。</p><p>在 JavaScript 中有两种劫持 property 访问的方式：<a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/get">getter</a> &#x2F; <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/set">setters</a> 和 <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy">Proxies</a>。Vue 2 使用 getter &#x2F; setters 完全是出于支持旧版本浏览器的限制。而在 Vue 3 中则使用了 Proxy 来创建响应式对象，仅将 getter &#x2F; setter 用于 ref。下面的伪代码将会说明它们是如何工作的：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">reactive</span>(<span class="params">obj</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Proxy</span>(obj, &#123;</span><br><span class="line">    <span class="title function_">get</span>(<span class="params">target, key</span>) &#123;</span><br><span class="line">      <span class="title function_">track</span>(target, key)</span><br><span class="line">      <span class="keyword">return</span> target[key]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="title function_">set</span>(<span class="params">target, key, value</span>) &#123;</span><br><span class="line">      target[key] = value</span><br><span class="line">      <span class="title function_">trigger</span>(target, key)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">ref</span>(<span class="params">value</span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> refObject = &#123;</span><br><span class="line">    <span class="keyword">get</span> <span class="title function_">value</span>() &#123;</span><br><span class="line">      <span class="title function_">track</span>(refObject, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">      <span class="keyword">return</span> value</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="keyword">set</span> <span class="title function_">value</span>(<span class="params">newValue</span>) &#123;</span><br><span class="line">      value = newValue</span><br><span class="line">      <span class="title function_">trigger</span>(refObject, <span class="string">&#x27;value&#x27;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> refObject</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上代码解释了我们在基础章节部分讨论过的一些 <a href="https://cn.vuejs.org/guide/essentials/reactivity-fundamentals.html#limitations-of-reactive"><code>reactive()</code> 的局限性</a>：</p><ul><li>当你将一个响应式对象的属性赋值或解构到一个本地变量时，访问或赋值该变量是非响应式的，因为它将不再触发源对象上的 get &#x2F; set 代理。注意这种“断开”只影响变量绑定——如果变量指向一个对象之类的非原始值，那么对该对象的修改仍然是响应式的。</li><li>从 <code>reactive()</code> 返回的代理尽管行为上表现得像原始对象，但我们通过使用 <code>===</code> 运算符还是能够比较出它们的不同。</li></ul><p>在 <code>track()</code> 内部，我们会检查当前是否有正在运行的副作用。如果有，我们会查找到一个存储了所有追踪了该属性的订阅者的 Set，然后将当前这个副作用作为新订阅者添加到该 Set 中。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这会在一个副作用就要运行之前被设置</span></span><br><span class="line"><span class="comment">// 我们会在后面处理它</span></span><br><span class="line"><span class="keyword">let</span> activeEffect</span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">track</span>(<span class="params">target, key</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (activeEffect) &#123;</span><br><span class="line">    <span class="keyword">const</span> effects = <span class="title function_">getSubscribersForProperty</span>(target, key)</span><br><span class="line">    effects.<span class="title function_">add</span>(activeEffect)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>副作用订阅将被存储在一个全局的 <code>WeakMap&lt;target, Map&lt;key, Set&lt;effect&gt;&gt;&gt;</code> 数据结构中。如果在第一次追踪时没有找到对相应属性订阅的副作用集合，它将会在这里新建。这就是 <code>getSubscribersForProperty()</code> 函数所做的事。为了简化描述，我们跳过了它其中的细节。</p><p>在 <code>trigger()</code> 之中，我们会再查找到该属性的所有订阅副作用。但这一次我们需要执行它们：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">trigger</span>(<span class="params">target, key</span>) &#123;</span><br><span class="line">  <span class="keyword">const</span> effects = <span class="title function_">getSubscribersForProperty</span>(target, key)</span><br><span class="line">  effects.<span class="title function_">forEach</span>(<span class="function">(<span class="params">effect</span>) =&gt;</span> <span class="title function_">effect</span>())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="单文件组件-Component可以理解为dom的一个标签"><a href="#单文件组件-Component可以理解为dom的一个标签" class="headerlink" title="单文件组件 Component可以理解为dom的一个标签"></a>单文件组件 Component可以理解为dom的一个标签</h1><p>Vue 的单文件组件 (即 <code>*.vue</code> 文件，英文 Single-File Component，简称 <strong>SFC</strong>) 是一种特殊的文件格式，使我们能够将一个 Vue 组件的模板、逻辑与样式封装在单个文件中。下面是一个单文件组件的示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;script&gt;</span><br><span class="line">export default &#123;</span><br><span class="line">  data() &#123;</span><br><span class="line">    return &#123;</span><br><span class="line">      greeting: &#x27;Hello World!&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;p class=&quot;greeting&quot;&gt;&#123;&#123; greeting &#125;&#125;&lt;/p&gt;</span><br><span class="line">&lt;/template&gt;</span><br><span class="line"></span><br><span class="line">&lt;style&gt;</span><br><span class="line">.greeting &#123;</span><br><span class="line">  color: red;</span><br><span class="line">  font-weight: bold;</span><br><span class="line">&#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/08/22/hello-world/"/>
      <url>/2023/08/22/hello-world/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试代码块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><p>测试md文件</p><p><strong>23年8月22日记</strong></p><h2 id="helloword"><a href="#helloword" class="headerlink" title="helloword"></a>helloword</h2><p>初始化博客</p><p>后续博客美化持续进行中</p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
