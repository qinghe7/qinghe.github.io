<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"qinghe7.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="source：Tianshou–强化学习算法框架学习笔记 - 知乎 (zhihu.com) Cheat Sheet — Tianshou 0.5.1 documentation 欢迎查看天授平台中文文档 — 天授 0.4.6.post1 文档 (tianshou.readthedocs.io) 一、Tianshou的基本框架天授（Tianshou）把一个RL训练流程划分成了几个子模块：traine">
<meta property="og:type" content="article">
<meta property="og:title" content="Tianshou">
<meta property="og:url" content="https://qinghe7.github.io/2023/08/27/Tianshou/index.html">
<meta property="og:site_name" content="Qinghe&#39;Blog">
<meta property="og:description" content="source：Tianshou–强化学习算法框架学习笔记 - 知乎 (zhihu.com) Cheat Sheet — Tianshou 0.5.1 documentation 欢迎查看天授平台中文文档 — 天授 0.4.6.post1 文档 (tianshou.readthedocs.io) 一、Tianshou的基本框架天授（Tianshou）把一个RL训练流程划分成了几个子模块：traine">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-44dbd0b48689e7816dd0701523d7d9be_720w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-d7073830e1c3abca3458de2d42f3a37e_720w.webp">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-1248cd71b07e48326fc65217097156f6_720w.webp">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-e0d5a938b90f41c1ddd9679ae5cad078_720w.webp">
<meta property="article:published_time" content="2023-08-27T12:22:29.000Z">
<meta property="article:modified_time" content="2023-08-27T12:33:16.565Z">
<meta property="article:author" content="Qinghe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp">

<link rel="canonical" href="https://qinghe7.github.io/2023/08/27/Tianshou/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Tianshou | Qinghe'Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Qinghe'Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你好，陌生人</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://qinghe7.github.io/2023/08/27/Tianshou/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/saber.png">
      <meta itemprop="name" content="Qinghe">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qinghe'Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tianshou
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2023-08-27 20:22:29 / 修改时间：20:33:16" itemprop="dateCreated datePublished" datetime="2023-08-27T20:22:29+08:00">2023-08-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforement-learning/" itemprop="url" rel="index"><span itemprop="name">Reinforement learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>16k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>15 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>source：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620988786">Tianshou–强化学习算法框架学习笔记 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://tianshou.readthedocs.io/en/master/tutorials/cheatsheet.html">Cheat Sheet — Tianshou 0.5.1 documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://tianshou.readthedocs.io/zh/master/">欢迎查看天授平台中文文档 — 天授 0.4.6.post1 文档 (tianshou.readthedocs.io)</a></p>
<h3 id="一、Tianshou的基本框架"><a href="#一、Tianshou的基本框架" class="headerlink" title="一、Tianshou的基本框架"></a><strong>一、Tianshou的基本框架</strong></h3><p>天授（Tianshou）把一个RL训练流程划分成了几个子模块：trainer（负责训练逻辑）、collector（负责数据采集）、policy（负责训练策略）和 buffer（负责数据存储），此外还有两个外围的模块，一个是env，一个是model（policy负责RL算法实现比如loss function的计算，model就只是个正常的神经网络）。下图描述了这些模块的依赖：</p>
<p><img src="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp" alt="https://pic2.zhimg.com/80/v2-da45fbece6e91c073061d6b0b82ae50d_720w.webp"></p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tianshou <span class="keyword">as</span> ts</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> tianshou.utils <span class="keyword">import</span> TensorboardLogger</span><br><span class="line"> ​</span><br><span class="line"> ​</span><br><span class="line"> <span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_shape, action_shape</span>):</span><br><span class="line">         <span class="built_in">super</span>().__init__()</span><br><span class="line">         self.model = nn.Sequential(</span><br><span class="line">             nn.Linear(np.prod(state_shape), <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, <span class="number">128</span>), nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">             nn.Linear(<span class="number">128</span>, np.prod(action_shape)),</span><br><span class="line">         )</span><br><span class="line"> ​</span><br><span class="line">     <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, obs, state=<span class="literal">None</span>, info=&#123;&#125;</span>):</span><br><span class="line">         <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obs, torch.Tensor):</span><br><span class="line">             obs = torch.tensor(obs, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">         batch = obs.shape[<span class="number">0</span>]</span><br><span class="line">         logits = self.model(obs.view(batch, -<span class="number">1</span>))</span><br><span class="line">         <span class="keyword">return</span> logits, state</span><br><span class="line"> ​</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Make an environment</span></span><br><span class="line"> env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line"> state_shape = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"> action_shape = env.action_space.n</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Build the Network</span></span><br><span class="line"> net = Net(state_shape, action_shape)</span><br><span class="line"> optim = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Setup Policy</span></span><br><span class="line"> policy = ts.policy.DQNPolicy(net, optim, discount_factor=<span class="number">0.9</span>, estimation_step=<span class="number">3</span>, target_update_freq=<span class="number">320</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Setup Collector</span></span><br><span class="line"> train_collector = ts.data.Collector(policy, env, ts.data.ReplayBuffer(<span class="number">20000</span>, <span class="number">10</span>), exploration_noise=<span class="literal">True</span>)</span><br><span class="line"> test_collector = ts.data.Collector(policy, env, exploration_noise=<span class="literal">True</span>)</span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># logging</span></span><br><span class="line"> <span class="comment"># writer = SummaryWriter(&#x27;log/dqn&#x27;)</span></span><br><span class="line"> <span class="comment"># logger = TensorboardLogger(writer)</span></span><br><span class="line"> ​</span><br><span class="line"> <span class="comment"># Train Policy with a Trainer</span></span><br><span class="line"> result = ts.trainer.offpolicy_trainer(</span><br><span class="line">     policy, train_collector, test_collector,</span><br><span class="line">     max_epoch=<span class="number">10</span>, step_per_epoch=<span class="number">10000</span>, step_per_collect=<span class="number">10</span>,</span><br><span class="line">     update_per_step=<span class="number">0.1</span>, episode_per_test=<span class="number">100</span>, batch_size=<span class="number">64</span>,</span><br><span class="line">     train_fn=<span class="keyword">lambda</span> epoch, env_step: policy.set_eps(<span class="number">0.1</span>),</span><br><span class="line">     test_fn=<span class="keyword">lambda</span> epoch, env_step: policy.set_eps(<span class="number">0.05</span>),</span><br><span class="line">     stop_fn=<span class="keyword">lambda</span> mean_rewards: mean_rewards &gt;= env.spec.reward_threshold)</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">f&#x27;Finished training! Use <span class="subst">&#123;result[<span class="string">&quot;duration&quot;</span>]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>下面，我们就将一步一步的了解上图中所有的API，对Tianshou有一个大致的了解。</p>
<h3 id="二、Batch"><a href="#二、Batch" class="headerlink" title="二、Batch"></a><strong>二、Batch</strong></h3><p>下面我们首先来看看Batch这个数据结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch</span><br><span class="line">data = Batch(a=<span class="number">4</span>, b=[<span class="number">5</span>, <span class="number">5</span>], c=<span class="string">&#x27;2312312&#x27;</span>, d=(<span class="string">&#x27;a&#x27;</span>, -<span class="number">2</span>, -<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="built_in">print</span>(data.b)</span><br><span class="line">​</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array(4),</span></span><br><span class="line"><span class="string">    b: array([5, 5]),</span></span><br><span class="line"><span class="string">    c: &#x27;2312312&#x27;,</span></span><br><span class="line"><span class="string">    d: array([&#x27;a&#x27;, &#x27;-2&#x27;, &#x27;-3&#x27;], dtype=object),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">[5 5]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以发现，batch类似于dict，存储key-value对，并且可以自动将value转化成numpy array。</p>
<p>下面例子演示Batch存储numpy和pytorch的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">batch1 = Batch(a=np.arange(<span class="number">2</span>), b=torch.zeros((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">batch2 = Batch(a=np.arange(<span class="number">2</span>), b=torch.ones((<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">batch_cat = Batch.cat([batch1, batch2, batch1])</span><br><span class="line"><span class="built_in">print</span>(batch1)</span><br><span class="line"><span class="built_in">print</span>(batch2)</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">    a: array([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>将Batch中的数据类型统一转换成numpy或pytorch的数据类型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">batch_cat.to_numpy()</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line">batch_cat.to_torch()</span><br><span class="line"><span class="built_in">print</span>(batch_cat)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: array([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">    b: array([[0., 0.],</span></span><br><span class="line"><span class="string">              [0., 0.],</span></span><br><span class="line"><span class="string">              [1., 1.],</span></span><br><span class="line"><span class="string">              [1., 1.],</span></span><br><span class="line"><span class="string">              [0., 0.],</span></span><br><span class="line"><span class="string">              [0., 0.]], dtype=float32),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">Batch(</span></span><br><span class="line"><span class="string">    a: tensor([0, 1, 0, 1, 0, 1]),</span></span><br><span class="line"><span class="string">    b: tensor([[0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [1., 1.],</span></span><br><span class="line"><span class="string">               [0., 0.],</span></span><br><span class="line"><span class="string">               [0., 0.]]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="三、ReplayBuffer"><a href="#三、ReplayBuffer" class="headerlink" title="三、ReplayBuffer"></a><strong>三、ReplayBuffer</strong></h3><p>Replay buffer在RL的off-policy中非常常用，其可以存储过去的经验，以便训练我们的agent。</p>
<p>在Tianshou中，可以把replay buffer看成一种特殊的Batch。</p>
<p>下面是使用replay buffer的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer</span><br><span class="line">​</span><br><span class="line"><span class="comment"># a buffer is initialised with its maxsize set to 10 (older data will be discarded if more data flow in).</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line">buf = ReplayBuffer(size=<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># add 3 steps of data into ReplayBuffer sequentially</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    buf.add(Batch(obs=i, act=i, rew=i, done=<span class="number">0</span>, obs_next=i + <span class="number">1</span>, info=&#123;&#125;))</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line">​</span><br><span class="line"><span class="comment"># add another 10 steps of data into ReplayBuffer sequentially</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, <span class="number">13</span>):</span><br><span class="line">    buf.add(Batch(obs=i, act=i, rew=i, done=<span class="number">0</span>, obs_next=i + <span class="number">1</span>, info=&#123;&#125;))</span><br><span class="line"><span class="built_in">print</span>(buf)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;maxsize: &#123;&#125;, data length: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(buf.maxsize, <span class="built_in">len</span>(buf)))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer()</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 0</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer(</span></span><br><span class="line"><span class="string">    info: Batch(),</span></span><br><span class="line"><span class="string">    obs_next: array([1, 2, 3, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    act: array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    obs: array([0, 1, 2, 0, 0, 0, 0, 0, 0, 0]),</span></span><br><span class="line"><span class="string">    done: array([False, False, False, False, False, False, False, False, False,</span></span><br><span class="line"><span class="string">                 False]),</span></span><br><span class="line"><span class="string">    rew: array([0., 1., 2., 0., 0., 0., 0., 0., 0., 0.]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 3</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">ReplayBuffer(</span></span><br><span class="line"><span class="string">    info: Batch(),</span></span><br><span class="line"><span class="string">    obs_next: array([11, 12, 13,  4,  5,  6,  7,  8,  9, 10]),</span></span><br><span class="line"><span class="string">    act: array([10, 11, 12,  3,  4,  5,  6,  7,  8,  9]),</span></span><br><span class="line"><span class="string">    obs: array([10, 11, 12,  3,  4,  5,  6,  7,  8,  9]),</span></span><br><span class="line"><span class="string">    done: array([False, False, False, False, False, False, False, False, False,</span></span><br><span class="line"><span class="string">                 False]),</span></span><br><span class="line"><span class="string">    rew: array([10., 11., 12.,  3.,  4.,  5.,  6.,  7.,  8.,  9.]),</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">maxsize: 10, data length: 10</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>replay buffer中保留了七个属性，Tianshou推荐我们使用这七个推荐的属性，而不是自己去创建其他属性。</p>
<p><img src="https://pic3.zhimg.com/80/v2-44dbd0b48689e7816dd0701523d7d9be_720w.webp"></p>
<p>我们也看到了，buffer其实就是一种特殊的Batch，那他存在的意义是什么呢？</p>
<p>就在于可以从buffer中sample数据给到collector中，供agent进行训练。</p>
<blockquote>
<p>现在Tianshou支持gymnasium，所以又多了两个属性：truncated和terminated。</p>
</blockquote>
<p>我们还可以高效的从buffer中追踪trajectory信息。</p>
<p>下面这段代码可以获得下标为6的step所处的episode的第一个step的下标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Search for the previous index of index &quot;6&quot;</span></span><br><span class="line">now_index = <span class="number">6</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    prev_index = buf.prev(now_index)</span><br><span class="line">    <span class="built_in">print</span>(prev_index)</span><br><span class="line">    <span class="keyword">if</span> prev_index == now_index:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        now_index = prev_index</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">5</span></span><br><span class="line"><span class="string">4</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>同理，下面代码可以返回在当前episode中下一个step的下标:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># next step of indexes [4,5,6,7,8,9] are:</span></span><br><span class="line"><span class="built_in">print</span>(buf.<span class="built_in">next</span>([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]))</span><br><span class="line"><span class="built_in">print</span>(buf.<span class="built_in">next</span>(<span class="number">7</span>))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[5 6 7 7 9 0]</span></span><br><span class="line"><span class="string">7</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>这在n-step-return的时候非常有用(n-step TD)</p>
<h3 id="四、Vectorized-Environment"><a href="#四、Vectorized-Environment" class="headerlink" title="四、Vectorized Environment"></a><strong>四、Vectorized Environment</strong></h3><p><img src="https://pic3.zhimg.com/80/v2-d7073830e1c3abca3458de2d42f3a37e_720w.webp"></p>
<p>在gym中，环境接收一个动作，返回下一个状态的观测和奖励。这个过程很慢，并且常常是实验的性能瓶颈，所以Tianshou利用并行环境加速这一过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> SubprocVectorEnv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">​</span><br><span class="line">num_cpus = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">for</span> num_cpu <span class="keyword">in</span> num_cpus:</span><br><span class="line">    <span class="comment"># SubprocVectorEnv这个wrapper利用多个进程，并行执行多个环境。</span></span><br><span class="line">    env = SubprocVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_cpu)])</span><br><span class="line">    env.reset()</span><br><span class="line">    sampled_steps = <span class="number">0</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line">    <span class="keyword">while</span> sampled_steps &lt; <span class="number">1000</span>:</span><br><span class="line">        act = np.random.choice(<span class="number">2</span>, size=num_cpu)</span><br><span class="line">        obs, rew, done, info = env.step(act)</span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">sum</span>(done):</span><br><span class="line">            env.reset(np.where(done)[<span class="number">0</span>])</span><br><span class="line">        sampled_steps += num_cpu</span><br><span class="line">    time_used = time.time() - time_start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;s used to sample 1000 steps if using &#123;&#125; cpus.&quot;</span>.<span class="built_in">format</span>(time_used, num_cpu))</span><br></pre></td></tr></table></figure>

<p>下面是单个环境与多个环境的对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="comment"># In Gym</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Tianshou</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">helper_function</span>():</span><br><span class="line">    env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">  <span class="comment"># other operations such as env.seed(np.random.choice(10))</span></span><br><span class="line">    <span class="keyword">return</span> env</span><br><span class="line">​</span><br><span class="line">envs = DummyVectorEnv([helper_function <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Gym, env.reset() returns a single observation.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In Gym, env.reset() returns a single observation.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(env.reset())</span><br><span class="line">​</span><br><span class="line"><span class="comment"># In Tianshou, envs.reset() returns stacked observations.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;========================================&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In Tianshou, envs.reset() returns stacked observations.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(envs.reset())</span><br><span class="line">​</span><br><span class="line">obs, rew, done, info = envs.step(np.random.choice(<span class="number">2</span>, size=num_cpu))</span><br><span class="line"><span class="built_in">print</span>(info)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">In Gym, env.reset() returns a single observation.</span></span><br><span class="line"><span class="string">[0.04703292 0.03945684 0.03802961 0.02598534]</span></span><br><span class="line"><span class="string">========================================</span></span><br><span class="line"><span class="string">In Tianshou, envs.reset() returns stacked observations.</span></span><br><span class="line"><span class="string">[[ 0.04029649 -0.01946092 -0.02980652 -0.01614117]</span></span><br><span class="line"><span class="string"> [-0.03085166 -0.04178732 -0.02325586  0.00156881]</span></span><br><span class="line"><span class="string"> [ 0.00672287  0.04306572  0.01217845 -0.04455359]</span></span><br><span class="line"><span class="string"> [ 0.03829754  0.02683093 -0.01153483  0.04290532]</span></span><br><span class="line"><span class="string"> [ 0.04420044  0.00097068 -0.01117315  0.04102308]]</span></span><br><span class="line"><span class="string">[&#123;&#x27;env_id&#x27;: 0&#125; &#123;&#x27;env_id&#x27;: 1&#125; &#123;&#x27;env_id&#x27;: 2&#125; &#123;&#x27;env_id&#x27;: 3&#125; &#123;&#x27;env_id&#x27;: 4&#125;]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="五、Policy"><a href="#五、Policy" class="headerlink" title="五、Policy"></a><strong>五、Policy</strong></h3><p>Policy就是agent如何做出action的\pi函数。</p>
<p>所有Policy模块都继承自BasePolicy类，并且具有相同的接口。</p>
<p>下面我们就来看看如何实现一个简单的REINFORCE的policy。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Dict</span>, <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Type</span>, <span class="type">Union</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer, to_torch, to_torch_as</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> BasePolicy</span><br><span class="line">​</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">REINFORCEPolicy</span>(<span class="title class_ inherited__">BasePolicy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implementation of REINFORCE algorithm.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br></pre></td></tr></table></figure>

<p>policy最重要的两个功能就是</p>
<ol>
<li>选择动作(forward)</li>
<li>更新参数(update)，update先调用process_fn函数，处理从buffer来的数据；然后调用learn，反向传播，更新参数。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Any</span>, <span class="type">Dict</span>, <span class="type">List</span>, <span class="type">Optional</span>, <span class="type">Type</span>, <span class="type">Union</span></span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Batch, ReplayBuffer, to_torch, to_torch_as</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> BasePolicy</span><br><span class="line">​</span><br><span class="line">​</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">REINFORCEPolicy</span>(<span class="title class_ inherited__">BasePolicy</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implementation of REINFORCE algorithm.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model: torch.nn.Module, optim: torch.optim.Optimizer,</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.actor = model</span><br><span class="line">        self.optim = optim</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, batch: Batch</span>) -&gt; Batch:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute action over the given batch data.&quot;&quot;&quot;</span></span><br><span class="line">        act = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> Batch(act=act)</span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_fn</span>(<span class="params">self, batch: Batch, buffer: ReplayBuffer, indices: np.ndarray</span>) -&gt; Batch:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Compute the discounted returns for each transition.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">​</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">learn</span>(<span class="params">self, batch: Batch, batch_size: <span class="built_in">int</span>, repeat: <span class="built_in">int</span></span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">List</span>[<span class="built_in">float</span>]]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Perform the back-propagation.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>

<h3 id="六、Collector"><a href="#六、Collector" class="headerlink" title="六、Collector"></a><strong>六、Collector</strong></h3><p>collector与policy和环境交互，在其内部，把envs和buffer有机的结合起来，封装了其中的数据交互。</p>
<p><img src="https://pic3.zhimg.com/80/v2-1248cd71b07e48326fc65217097156f6_720w.webp"></p>
<p>Collector在训练（收集数据）时和评估策略时都可以使用。</p>
<p>Data Collecting：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> VectorReplayBuffer</span><br><span class="line">​</span><br><span class="line">train_env_num = <span class="number">4</span></span><br><span class="line">buffer_size = <span class="number">100</span></span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v0&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(train_env_num)])</span><br><span class="line">replaybuffer = VectorReplayBuffer(buffer_size, train_env_num)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 定义一个Collector</span></span><br><span class="line">train_collector = Collector(policy, train_envs, replaybuffer)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 利用Collector收集50个step的数据，自动存入replaybuffer中</span></span><br><span class="line">collect_result = train_collector.collect(n_step=<span class="number">50</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面我们可以从buffer中抽样数据</span></span><br><span class="line">replaybuffer.sample(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>Policy evaluation：</p>
<p>我们已经有了一个policy，现在我们想评估一下这个policy，看看reward情况等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PGPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v0&quot;</span>)</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v0&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># model</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">16</span>,])</span><br><span class="line">actor = Actor(net, env.action_space.shape)</span><br><span class="line">optim = torch.optim.Adam(actor.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line">​</span><br><span class="line">policy = PGPolicy(actor, optim, dist_fn=torch.distributions.Categorical)</span><br><span class="line">test_collector = Collector(policy, test_envs)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 收集9个episode</span></span><br><span class="line">collect_result = test_collector.collect(n_episode=<span class="number">9</span>)</span><br><span class="line"><span class="built_in">print</span>(collect_result)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;&#x27;n/ep&#x27;: 9, &#x27;n/st&#x27;: 82, &#x27;rews&#x27;: array([ 9.,  9.,  9.,  9.,  8.,  9.,  9., 11.,  9.]), &#x27;lens&#x27;: array([ 9,  9,  9,  9,  8,  9,  9, 11,  9]), &#x27;idxs&#x27;: array([0, 1, 0, 1, 0, 1, 0, 1, 0]), &#x27;rew&#x27;: 9.11111111111111, &#x27;len&#x27;: 9.11111111111111, &#x27;rew_std&#x27;: 0.7370277311900889, &#x27;len_std&#x27;: 0.7370277311900889&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Rewards of 9 episodes are &#123;&#125;&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;rews&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Rewards of 9 episodes are [ 9.  9.  9.  9.  8.  9.  9. 11.  9.]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average episode reward is &#123;&#125;.&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;rew&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Average episode reward is 9.11111111111111.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average episode length is &#123;&#125;.&quot;</span>.<span class="built_in">format</span>(collect_result[<span class="string">&quot;len&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Average episode length is 9.11111111111111.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="七、Trainer"><a href="#七、Trainer" class="headerlink" title="七、Trainer"></a><strong>七、Trainer</strong></h3><p>Trainer是Tianshou中的顶层封装，它控制traning loop和对Policy的evaluation。Trainer控制Policy和Collector的交互。</p>
<p>Tianshou中包含三类Trainer：On-policy training, off-policy training, offline training.</p>
<p>下面是REINFORCE算法的整体流程(利用On-policy)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector, VectorReplayBuffer</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PGPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">​</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">train_env_num = <span class="number">4</span></span><br><span class="line">buffer_size = <span class="number">2000</span>  <span class="comment"># Since REINFORCE is an on-policy algorithm, we don&#x27;t need a very large buffer size</span></span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the environments, used for training and evaluation</span></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>)</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&quot;CartPole-v1&quot;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(train_env_num)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the Policy instance</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">16</span>, ])</span><br><span class="line">actor = Actor(net, env.action_space.shape)</span><br><span class="line">optim = torch.optim.Adam(actor.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">policy = PGPolicy(actor, optim, dist_fn=torch.distributions.Categorical)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># Create the replay buffer and the collector</span></span><br><span class="line">replaybuffer = VectorReplayBuffer(buffer_size, train_env_num)</span><br><span class="line">test_collector = Collector(policy, test_envs)  <span class="comment"># 可以发现，test_collector没有replaybuffer，因为不做训练，只是测试</span></span><br><span class="line">train_collector = Collector(policy, train_envs, replaybuffer)</span><br><span class="line">​</span><br><span class="line">train_collector.reset()</span><br><span class="line">train_envs.reset()</span><br><span class="line">test_collector.reset()</span><br><span class="line">test_envs.reset()</span><br><span class="line">replaybuffer.reset()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):  <span class="comment"># 10 epoch</span></span><br><span class="line">    evaluation_result = test_collector.collect(n_episode=<span class="number">10</span>)  <span class="comment"># test_collector用来测试当前policy，得出reward。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evaluation reward is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(evaluation_result[<span class="string">&quot;rew&quot;</span>]))</span><br><span class="line">    train_collector.collect(n_step=<span class="number">2000</span>)  <span class="comment"># 收集2000个step到replaybuffer中</span></span><br><span class="line">    <span class="comment"># 0 means taking all data stored in train_collector.buffer</span></span><br><span class="line">    policy.update(<span class="number">0</span>, train_collector.buffer, batch_size=<span class="number">512</span>, repeat=<span class="number">1</span>)  <span class="comment"># buffer中所有数据，每次batch_size为512，</span></span><br><span class="line">    train_collector.reset_buffer(keep_statistics=<span class="literal">True</span>)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># 下面是使用tainer的代码：</span></span><br><span class="line">train_collector.reset()</span><br><span class="line">train_envs.reset()</span><br><span class="line">test_collector.reset()</span><br><span class="line">test_envs.reset()</span><br><span class="line">replaybuffer.reset()</span><br><span class="line">​</span><br><span class="line">result = onpolicy_trainer(</span><br><span class="line">    policy,</span><br><span class="line">    train_collector,</span><br><span class="line">    test_collector,</span><br><span class="line">    max_epoch=<span class="number">10</span>,</span><br><span class="line">    step_per_epoch=<span class="number">1</span>,  <span class="comment"># 每个epoch进行多少次transitions</span></span><br><span class="line">    repeat_per_collect=<span class="number">1</span>, <span class="comment"># the number of repeat time for policy learning, for example, set it to 2 means the policy needs to learn each given batch data twice.</span></span><br><span class="line">    episode_per_test=<span class="number">10</span>,  <span class="comment"># 每次测试进行几个episode</span></span><br><span class="line">    step_per_collect=<span class="number">2000</span>, <span class="comment"># 每次update前，收集多少step的数据</span></span><br><span class="line">    batch_size=<span class="number">512</span>,  <span class="comment"># update的时候batch的大小</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<p>可以发现，trainer就是包装了一下循环。</p>
<h3 id="八、Experiment"><a href="#八、Experiment" class="headerlink" title="八、Experiment"></a><strong>八、Experiment</strong></h3><p>这一节我们用PPO来解决CartPole</p>
<p><img src="https://pic1.zhimg.com/80/v2-e0d5a938b90f41c1ddd9679ae5cad078_720w.webp"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">​</span><br><span class="line"><span class="keyword">from</span> tianshou.data <span class="keyword">import</span> Collector, VectorReplayBuffer</span><br><span class="line"><span class="keyword">from</span> tianshou.env <span class="keyword">import</span> DummyVectorEnv</span><br><span class="line"><span class="keyword">from</span> tianshou.policy <span class="keyword">import</span> PPOPolicy</span><br><span class="line"><span class="keyword">from</span> tianshou.trainer <span class="keyword">import</span> onpolicy_trainer</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.common <span class="keyword">import</span> ActorCritic, Net</span><br><span class="line"><span class="keyword">from</span> tianshou.utils.net.discrete <span class="keyword">import</span> Actor, Critic</span><br><span class="line">​</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">​</span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">​</span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>)</span><br><span class="line">train_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)])</span><br><span class="line">test_envs = DummyVectorEnv([<span class="keyword">lambda</span>: gym.make(<span class="string">&#x27;CartPole-v0&#x27;</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>)])</span><br><span class="line">​</span><br><span class="line"><span class="comment"># net is the shared head of the actor and the critic</span></span><br><span class="line">net = Net(env.observation_space.shape, hidden_sizes=[<span class="number">64</span>, <span class="number">64</span>], device=device)</span><br><span class="line">actor = Actor(net, env.action_space.n, device=device).to(device)</span><br><span class="line">critic = Critic(net, device=device).to(device)</span><br><span class="line">actor_critic = ActorCritic(actor, critic)</span><br><span class="line">​</span><br><span class="line"><span class="comment"># optimizer of the actor and the critic</span></span><br><span class="line">optim = torch.optim.Adam(actor_critic.parameters(), lr=<span class="number">0.0003</span>)</span><br><span class="line">​</span><br><span class="line">dist = torch.distributions.Categorical</span><br><span class="line">policy = PPOPolicy(actor, critic, optim, dist, action_space=env.action_space, deterministic_eval=<span class="literal">True</span>)</span><br><span class="line">​</span><br><span class="line">train_collector = Collector(policy, train_envs, VectorReplayBuffer(<span class="number">20000</span>, <span class="built_in">len</span>(train_envs)))</span><br><span class="line">test_collector = Collector(policy, test_envs)</span><br><span class="line">​</span><br><span class="line">result = onpolicy_trainer(</span><br><span class="line">    policy,</span><br><span class="line">    train_collector,</span><br><span class="line">    test_collector,</span><br><span class="line">    max_epoch=<span class="number">10</span>,</span><br><span class="line">    step_per_epoch=<span class="number">50000</span>,</span><br><span class="line">    repeat_per_collect=<span class="number">10</span>,</span><br><span class="line">    episode_per_test=<span class="number">10</span>,</span><br><span class="line">    batch_size=<span class="number">256</span>,</span><br><span class="line">    step_per_collect=<span class="number">2000</span>,</span><br><span class="line">    stop_fn=<span class="keyword">lambda</span> mean_reward: mean_reward &gt;= <span class="number">195</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Qinghe
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://qinghe7.github.io/2023/08/27/Tianshou/" title="Tianshou">https://qinghe7.github.io/2023/08/27/Tianshou/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/08/22/vue/" rel="prev" title="My New Post">
      <i class="fa fa-chevron-left"></i> My New Post
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/08/30/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E8%AE%B0/" rel="next" title="个人随记">
      个人随记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81Tianshou%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="nav-number">1.</span> <span class="nav-text">一、Tianshou的基本框架</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Batch"><span class="nav-number">2.</span> <span class="nav-text">二、Batch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81ReplayBuffer"><span class="nav-number">3.</span> <span class="nav-text">三、ReplayBuffer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Vectorized-Environment"><span class="nav-number">4.</span> <span class="nav-text">四、Vectorized Environment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%94%E3%80%81Policy"><span class="nav-number">5.</span> <span class="nav-text">五、Policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%AD%E3%80%81Collector"><span class="nav-number">6.</span> <span class="nav-text">六、Collector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%83%E3%80%81Trainer"><span class="nav-number">7.</span> <span class="nav-text">七、Trainer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%AB%E3%80%81Experiment"><span class="nav-number">8.</span> <span class="nav-text">八、Experiment</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Qinghe"
      src="/images/saber.png">
  <p class="site-author-name" itemprop="name">Qinghe</p>
  <div class="site-description" itemprop="description">选择有时候比努力更重要</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/qinghe7" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qinghe7" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qinghe</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">24k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">22 分钟</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
